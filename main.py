import re
import uuid
import json
import os
import aiohttp
import asyncio
import base64
import astrbot.api.message_components as Comp
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from astrbot.api.event import filter, AstrMessageEvent, MessageChain
from astrbot.api.star import Context, Star, register
from astrbot.api import logger, AstrBotConfig

GITHUB_URL_PATTERN = r"https://github\.com/[\w\-]+/[\w\-]+(?:/(pull|issues)/\d+)?"
GITHUB_REPO_OPENGRAPH = "https://opengraph.githubassets.com/{hash}/{appendix}"
STAR_HISTORY_URL = "https://api.star-history.com/svg?repos={identifier}&type=Date"
GITHUB_API_URL = "https://api.github.com/repos/{repo}"
GITHUB_README_API_URL = "https://api.github.com/repos/{repo}/readme"  # æ–°å¢ README API URL
GITHUB_ISSUES_API_URL = "https://api.github.com/repos/{repo}/issues"
GITHUB_ISSUE_API_URL = "https://api.github.com/repos/{repo}/issues/{issue_number}"
GITHUB_PR_API_URL = "https://api.github.com/repos/{repo}/pulls/{pr_number}"
GITHUB_RATE_LIMIT_URL = "https://api.github.com/rate_limit"

# Path for storing subscription data
SUBSCRIPTION_FILE = "data/github_subscriptions.json"
# Path for storing default repo data
DEFAULT_REPO_FILE = "data/github_default_repos.json"


@register(
    "astrbot_plugin_github_cards",
    "Soulter",
    "æ ¹æ®ç¾¤èŠä¸­ GitHub ç›¸å…³é“¾æ¥è‡ªåŠ¨å‘é€ GitHub OpenGraph å›¾ç‰‡ï¼Œæ”¯æŒè®¢é˜…ä»“åº“çš„ Issue å’Œ PR",
    "1.0.2", 
    "https://github.com/Soulter/astrbot_plugin_github_cards",
)
class MyPlugin(Star):
    def __init__(self, context: Context, config: AstrBotConfig = None):
        super().__init__(context)
        self.config = config or {}
        self.subscriptions = self._load_subscriptions()
        self.default_repos = self._load_default_repos()
        self.last_check_time = {}  # Store the last check time for each repo
        self.use_lowercase = self.config.get("use_lowercase_repo", True)
        self.github_token = self.config.get("github_token", "")
        self.check_interval = self.config.get("check_interval", 30)

        # Start background task to check for updates
        self.task = asyncio.create_task(self._check_updates_periodically())
        logger.info(
            f"GitHub Cards Pluginåˆå§‹åŒ–å®Œæˆï¼Œæ£€æŸ¥é—´éš”: {self.check_interval}åˆ†é’Ÿ"
        )

    # ... (ä¿ç•™æ‰€æœ‰ç°æœ‰æ–¹æ³• _load_subscriptions åˆ° _format_rate_limit)
    def _load_subscriptions(self) -> Dict[str, List[str]]:
        """Load subscriptions from JSON file"""
        if os.path.exists(SUBSCRIPTION_FILE):
            try:
                with open(SUBSCRIPTION_FILE, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"åŠ è½½è®¢é˜…æ•°æ®å¤±è´¥: {e}")
        return {}

    def _save_subscriptions(self):
        """Save subscriptions to JSON file"""
        try:
            os.makedirs(os.path.dirname(SUBSCRIPTION_FILE), exist_ok=True)
            with open(SUBSCRIPTION_FILE, "w", encoding="utf-8") as f:
                json.dump(self.subscriptions, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜è®¢é˜…æ•°æ®å¤±è´¥: {e}")

    def _load_default_repos(self) -> Dict[str, str]:
        """Load default repo settings from JSON file"""
        if os.path.exists(DEFAULT_REPO_FILE):
            try:
                with open(DEFAULT_REPO_FILE, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"åŠ è½½é»˜è®¤ä»“åº“æ•°æ®å¤±è´¥: {e}")
        return {}

    def _save_default_repos(self):
        """Save default repo settings to JSON file"""
        try:
            os.makedirs(os.path.dirname(DEFAULT_REPO_FILE), exist_ok=True)
            with open(DEFAULT_REPO_FILE, "w", encoding="utf-8") as f:
                json.dump(self.default_repos, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜é»˜è®¤ä»“åº“æ•°æ®å¤±è´¥: {e}")

    def _normalize_repo_name(self, repo: str) -> str:
        """Normalize repository name according to configuration"""
        return repo.lower() if self.use_lowercase else repo

    def _get_github_headers(self) -> Dict[str, str]:
        """Get GitHub API headers with token if available"""
        headers = {"Accept": "application/vnd.github.v3+json"}
        if self.github_token:
            headers["Authorization"] = f"token {self.github_token}"
        return headers

    @filter.regex(GITHUB_URL_PATTERN)
    async def github_repo(self, event: AstrMessageEvent):
        """è§£æ Github ä»“åº“ä¿¡æ¯"""
        msg = event.message_str
        match = re.search(GITHUB_URL_PATTERN, msg)
        repo_url = match.group(0)
        repo_url = repo_url.replace("https://github.com/", "")
        hash_value = uuid.uuid4().hex
        opengraph_url = GITHUB_REPO_OPENGRAPH.format(hash=hash_value, appendix=repo_url)
        logger.info(f"ç”Ÿæˆçš„ OpenGraph URL: {opengraph_url}")

        try:
            yield event.image_result(opengraph_url)
        except Exception as e:
            logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}")
            yield event.plain_result("ä¸‹è½½ GitHub å›¾ç‰‡å¤±è´¥: " + str(e))
            return

    @filter.command("ghsub")
    async def subscribe_repo(self, event: AstrMessageEvent, repo: str):
        """è®¢é˜… GitHub ä»“åº“çš„ Issue å’Œ PRã€‚ä¾‹å¦‚: /ghsub Soulter/AstrBot"""
        if not self._is_valid_repo(repo):
            yield event.plain_result("è¯·æä¾›æœ‰æ•ˆçš„ä»“åº“åï¼Œæ ¼å¼ä¸º: ç”¨æˆ·å/ä»“åº“å")
            return

        # Normalize repository name
        normalized_repo = self._normalize_repo_name(repo)

        # Check if the repo exists
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    GITHUB_API_URL.format(repo=repo), headers=self._get_github_headers()
                ) as resp:
                    if resp.status != 200:
                        yield event.plain_result(f"ä»“åº“ {repo} ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®")
                        return

                    repo_data = await resp.json()
                    display_name = repo_data.get("full_name", repo)
        except Exception as e:
            logger.error(f"è®¿é—® GitHub API å¤±è´¥: {e}")
            yield event.plain_result(f"æ£€æŸ¥ä»“åº“æ—¶å‡ºé”™: {str(e)}")
            return

        # Get the unique identifier for the subscriber
        subscriber_id = event.unified_msg_origin

        # Add or update subscription
        if normalized_repo not in self.subscriptions:
            self.subscriptions[repo] = []

        if subscriber_id not in self.subscriptions[repo]:
            self.subscriptions[repo].append(subscriber_id)
            self._save_subscriptions()

            # Fetch initial state for new subscription
            await self._fetch_new_items(normalized_repo, None)

            yield event.plain_result(f"æˆåŠŸè®¢é˜…ä»“åº“ {display_name} çš„ Issue å’Œ PR æ›´æ–°")
        else:
            yield event.plain_result(f"ä½ å·²ç»è®¢é˜…äº†ä»“åº“ {display_name}")

        # Set as default repo for this conversation
        self.default_repos[event.unified_msg_origin] = repo
        self._save_default_repos()

    @filter.command("ghunsub")
    async def unsubscribe_repo(self, event: AstrMessageEvent, repo: str = None):
        """å–æ¶ˆè®¢é˜… GitHub ä»“åº“ã€‚ä¾‹å¦‚: /ghunsub Soulter/AstrBotï¼Œä¸æä¾›ä»“åº“ååˆ™å–æ¶ˆæ‰€æœ‰è®¢é˜…"""
        subscriber_id = event.unified_msg_origin

        if repo is None:
            # Unsubscribe from all repos
            unsubscribed = []
            for repo_name, subscribers in list(self.subscriptions.items()):
                if subscriber_id in subscribers:
                    subscribers.remove(subscriber_id)
                    unsubscribed.append(repo_name)
                    if not subscribers:
                        del self.subscriptions[repo_name]

            if unsubscribed:
                self._save_subscriptions()
                yield event.plain_result(
                    f"å·²å–æ¶ˆè®¢é˜…æ‰€æœ‰ä»“åº“: {', '.join(unsubscribed)}"
                )
            else:
                yield event.plain_result("ä½ æ²¡æœ‰è®¢é˜…ä»»ä½•ä»“åº“")
            return

        if not self._is_valid_repo(repo):
            yield event.plain_result("è¯·æä¾›æœ‰æ•ˆçš„ä»“åº“åï¼Œæ ¼å¼ä¸º: ç”¨æˆ·å/ä»“åº“å")
            return

        # Normalize repository name
        normalized_repo = self._normalize_repo_name(repo)

        # Find the repo case-insensitively if using lowercase
        if self.use_lowercase:
            matched_repos = [
                r for r in self.subscriptions.keys() if r.lower() == normalized_repo
            ]
            if matched_repos:
                normalized_repo = matched_repos[0]

        if (
            normalized_repo in self.subscriptions
            and subscriber_id in self.subscriptions[normalized_repo]
        ):
            self.subscriptions[normalized_repo].remove(subscriber_id)
            if not self.subscriptions[normalized_repo]:
                del self.subscriptions[normalized_repo]
            self._save_subscriptions()
            yield event.plain_result(f"å·²å–æ¶ˆè®¢é˜…ä»“åº“ {repo}")
        else:
            yield event.plain_result(f"ä½ æ²¡æœ‰è®¢é˜…ä»“åº“ {repo}")

    @filter.command("ghlist")
    async def list_subscriptions(self, event: AstrMessageEvent):
        """åˆ—å‡ºå½“å‰è®¢é˜…çš„ GitHub ä»“åº“"""
        subscriber_id = event.unified_msg_origin
        subscribed_repos = []

        for repo, subscribers in self.subscriptions.items():
            if subscriber_id in subscribers:
                subscribed_repos.append(repo)

        if subscribed_repos:
            yield event.plain_result(
                f"ä½ å½“å‰è®¢é˜…çš„ä»“åº“æœ‰: {', '.join(subscribed_repos)}"
            )
        else:
            yield event.plain_result("ä½ å½“å‰æ²¡æœ‰è®¢é˜…ä»»ä½•ä»“åº“")

    @filter.command("ghdefault", alias={"ghdef"})
    async def set_default_repo(self, event: AstrMessageEvent, repo: str = None):
        """è®¾ç½®é»˜è®¤ä»“åº“ã€‚ä¾‹å¦‚: /ghdefault Soulter/AstrBot"""
        if repo is None:
            # Show current default repo
            default_repo = self.default_repos.get(event.unified_msg_origin)
            if default_repo:
                yield event.plain_result(f"å½“å‰é»˜è®¤ä»“åº“ä¸º: {default_repo}")
            else:
                yield event.plain_result(
                    "å½“å‰æœªè®¾ç½®é»˜è®¤ä»“åº“ï¼Œå¯ä½¿ç”¨ /ghdefault ç”¨æˆ·å/ä»“åº“å è¿›è¡Œè®¾ç½®"
                )
            return

        if not self._is_valid_repo(repo):
            yield event.plain_result("è¯·æä¾›æœ‰æ•ˆçš„ä»“åº“åï¼Œæ ¼å¼ä¸º: ç”¨æˆ·å/ä»“åº“å")
            return

        # Check if the repo exists
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    GITHUB_API_URL.format(repo=repo), headers=self._get_github_headers()
                ) as resp:
                    if resp.status != 200:
                        yield event.plain_result(f"ä»“åº“ {repo} ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®")
                        return

                    repo_data = await resp.json()
                    display_name = repo_data.get("full_name", repo)
        except Exception as e:
            logger.error(f"è®¿é—® GitHub API å¤±è´¥: {e}")
            yield event.plain_result(f"æ£€æŸ¥ä»“åº“æ—¶å‡ºé”™: {str(e)}")
            return

        # Set as default repo for this conversation
        self.default_repos[event.unified_msg_origin] = repo
        self._save_default_repos()
        yield event.plain_result(f"å·²å°† {display_name} è®¾ä¸ºé»˜è®¤ä»“åº“")

    def _is_valid_repo(self, repo: str) -> bool:
        """Check if the repository name is valid"""
        return bool(re.match(r"[\w\-]+/[\w\-]+$", repo))

    async def _check_updates_periodically(self):
        """Periodically check for updates in subscribed repositories"""
        try:
            while True:
                try:
                    await self._check_all_repos()
                except Exception as e:
                    logger.error(f"æ£€æŸ¥ä»“åº“æ›´æ–°æ—¶å‡ºé”™: {e}")

                # Use configured check interval
                minutes = max(1, self.check_interval)  # Ensure at least 1 minute
                logger.debug(f"ç­‰å¾… {minutes} åˆ†é’Ÿåå†æ¬¡æ£€æŸ¥ä»“åº“æ›´æ–°")
                await asyncio.sleep(minutes * 60)
        except asyncio.CancelledError:
            logger.info("åœæ­¢æ£€æŸ¥ä»“åº“æ›´æ–°")

    async def _check_all_repos(self):
        """Check all subscribed repositories for updates"""
        for repo in list(self.subscriptions.keys()):
            logger.info(f"æ­£åœ¨æ£€æŸ¥ä»“åº“ {repo} æ›´æ–°")
            if not self.subscriptions[repo]:  # Skip if no subscribers
                continue

            try:
                # Get the last check time for this repo
                last_check = self.last_check_time.get(repo, None)

                # Fetch new issues and PRs
                new_items = await self._fetch_new_items(repo, last_check)

                if new_items:
                    # Update last check time
                    self.last_check_time[repo] = datetime.now().isoformat()

                    # Notify subscribers about new items
                    await self._notify_subscribers(repo, new_items)
            except Exception as e:
                logger.error(f"æ£€æŸ¥ä»“åº“ {repo} æ›´æ–°æ—¶å‡ºé”™: {e}")

    async def _fetch_new_items(self, repo: str, last_check: str):
        """Fetch new issues and PRs from a repository since last check"""
        if not last_check:
            # If first time checking, just record current time and return empty list
            # Store as UTC timestamp without timezone info to avoid comparison issues
            self.last_check_time[repo] = (
                datetime.utcnow().replace(microsecond=0).isoformat()
            )
            logger.info(f"åˆå§‹åŒ–ä»“åº“ {repo} çš„æ—¶é—´æˆ³: {self.last_check_time[repo]}")
            return []

        try:
            # Always treat stored timestamps as UTC without timezone info
            last_check_dt = datetime.fromisoformat(last_check)

            # Ensure it's treated as naive datetime
            if hasattr(last_check_dt, "tzinfo") and last_check_dt.tzinfo is not None:
                # If it somehow has timezone info, convert to naive UTC
                last_check_dt = last_check_dt.replace(tzinfo=None)

            logger.info(f"ä»“åº“ {repo} çš„ä¸Šæ¬¡æ£€æŸ¥æ—¶é—´: {last_check_dt.isoformat()}")
            new_items = []

            # GitHub API returns both issues and PRs in the issues endpoint
            async with aiohttp.ClientSession() as session:
                try:
                    params = {
                        "sort": "created",
                        "direction": "desc",
                        "state": "all",
                        "per_page": 10,
                    }
                    async with session.get(
                        GITHUB_ISSUES_API_URL.format(repo=repo),
                        params=params,
                        headers=self._get_github_headers(),
                    ) as resp:
                        if resp.status == 200:
                            items = await resp.json()

                            for item in items:
                                # Convert GitHub's timestamp to naive UTC datetime for consistent comparison
                                github_timestamp = item["created_at"].replace("Z", "")
                                created_at = datetime.fromisoformat(github_timestamp)

                                # Always remove timezone info for comparison
                                created_at = created_at.replace(tzinfo=None)

                                logger.info(
                                    f"æ¯”è¾ƒ: ä»“åº“ {repo} çš„ item #{item['number']} åˆ›å»ºäº {created_at.isoformat()}, ä¸Šæ¬¡æ£€æŸ¥: {last_check_dt.isoformat()}"
                                )

                                if created_at > last_check_dt:
                                    logger.info(
                                        f"å‘ç°æ–°çš„ item #{item['number']} in {repo}"
                                    )
                                    new_items.append(item)
                                else:
                                    # Since items are sorted by creation time, we can break early
                                    logger.info(f"æ²¡æœ‰æ›´å¤šæ–° items in {repo}")
                                    break
                        else:
                            logger.error(
                                f"è·å–ä»“åº“ {repo} çš„ Issue/PR å¤±è´¥: {resp.status}: {await resp.text()}"
                            )
                except Exception as e:
                    logger.error(f"è·å–ä»“åº“ {repo} çš„ Issue/PR æ—¶å‡ºé”™: {e}")

            # Update the last check time to now (UTC without timezone info)
            if new_items:
                logger.info(f"æ‰¾åˆ° {len(new_items)} ä¸ªæ–°çš„ items åœ¨ {repo}")
            else:
                logger.info(f"æ²¡æœ‰æ‰¾åˆ°æ–°çš„ items åœ¨ {repo}")

            # Always update the timestamp after checking, regardless of whether we found items
            self.last_check_time[repo] = (
                datetime.utcnow().replace(microsecond=0).isoformat()
            )
            logger.info(f"æ›´æ–°ä»“åº“ {repo} çš„æ—¶é—´æˆ³ä¸º: {self.last_check_time[repo]}")

            return new_items
        except Exception as e:
            logger.error(f"è§£ææ—¶é—´æ—¶å‡ºé”™: {e}")
            # If we can't parse the time correctly, just return an empty list
            # and update the last check time to prevent continuous errors
            self.last_check_time[repo] = (
                datetime.utcnow().replace(microsecond=0).isoformat()
            )
            logger.info(
                f"å‡ºé”™åæ›´æ–°ä»“åº“ {repo} çš„æ—¶é—´æˆ³ä¸º: {self.last_check_time[repo]}"
            )
            return []

    async def _notify_subscribers(self, repo: str, new_items: List[Dict]):
        """Notify subscribers about new issues and PRs"""
        if not new_items:
            return

        for subscriber_id in self.subscriptions.get(repo, []):
            try:
                # Create notification message
                for item in new_items:
                    item_type = "PR" if "pull_request" in item else "Issue"
                    message = (
                        f"[GitHub æ›´æ–°] ä»“åº“ {repo} æœ‰æ–°çš„{item_type}:\n"
                        f"#{item['number']} {item['title']}\n"
                        f"ä½œè€…: {item['user']['login']}\n"
                        f"é“¾æ¥: {item['html_url']}"
                    )

                    # Send message to subscriber
                    await self.context.send_message(
                        subscriber_id, MessageChain(chain=[Comp.Plain(message)])
                    )

                    # Add a small delay between messages to avoid rate limiting
                    await asyncio.sleep(1)
            except Exception as e:
                logger.error(f"å‘è®¢é˜…è€… {subscriber_id} å‘é€é€šçŸ¥æ—¶å‡ºé”™: {e}")

    @filter.command("ghissue", alias={"ghis"})
    async def get_issue_details(self, event: AstrMessageEvent, issue_ref: str):
        """è·å– GitHub Issue è¯¦æƒ…ã€‚æ ¼å¼ï¼š/ghissue ç”¨æˆ·å/ä»“åº“å#123 æˆ– /ghissue 123 (ä½¿ç”¨é»˜è®¤ä»“åº“)"""
        repo, issue_number = self._parse_issue_reference(
            issue_ref, event.unified_msg_origin
        )
        if not repo or not issue_number:
            yield event.plain_result(
                "è¯·æä¾›æœ‰æ•ˆçš„ Issue å¼•ç”¨ï¼Œæ ¼å¼ä¸ºï¼šç”¨æˆ·å/ä»“åº“å#123 æˆ–çº¯æ•°å­—(ä½¿ç”¨é»˜è®¤ä»“åº“)"
            )
            return

        try:
            issue_data = await self._fetch_issue_data(repo, issue_number)
            if not issue_data:
                yield event.plain_result(
                    f"æ— æ³•è·å– Issue {repo}#{issue_number} çš„ä¿¡æ¯ï¼Œå¯èƒ½ä¸å­˜åœ¨æˆ–æ— è®¿é—®æƒé™"
                )
                return

            # Format and send the issue details
            result = self._format_issue_details(repo, issue_data)
            yield event.plain_result(result)

            # Send the issue card image if available
            if issue_data.get("html_url"):
                hash_value = uuid.uuid4().hex
                url_path = issue_data["html_url"].replace("https://github.com/", "")
                card_url = GITHUB_REPO_OPENGRAPH.format(
                    hash=hash_value, appendix=url_path
                )
                try:
                    yield event.image_result(card_url)
                except Exception as e:
                    logger.error(f"ä¸‹è½½ Issue å¡ç‰‡å›¾ç‰‡å¤±è´¥: {e}")

        except Exception as e:
            logger.error(f"è·å– Issue è¯¦æƒ…æ—¶å‡ºé”™: {e}")
            yield event.plain_result(f"è·å– Issue è¯¦æƒ…æ—¶å‡ºé”™: {str(e)}")

    @filter.command("ghpr")
    async def get_pr_details(self, event: AstrMessageEvent, pr_ref: str):
        """è·å– GitHub PR è¯¦æƒ…ã€‚æ ¼å¼ï¼š/ghpr ç”¨æˆ·å/ä»“åº“å#123 æˆ– /ghpr 123 (ä½¿ç”¨é»˜è®¤ä»“åº“)"""
        repo, pr_number = self._parse_issue_reference(pr_ref, event.unified_msg_origin)
        if not repo or not pr_number:
            yield event.plain_result(
                "è¯·æä¾›æœ‰æ•ˆçš„ PR å¼•ç”¨ï¼Œæ ¼å¼ä¸ºï¼šç”¨æˆ·å/ä»“åº“å#123 æˆ–çº¯æ•°å­—(ä½¿ç”¨é»˜è®¤ä»“åº“)"
            )
            return

        try:
            pr_data = await self._fetch_pr_data(repo, pr_number)
            if not pr_data:
                yield event.plain_result(
                    f"æ— æ³•è·å– PR {repo}#{pr_number} çš„ä¿¡æ¯ï¼Œå¯èƒ½ä¸å­˜åœ¨æˆ–æ— è®¿é—®æƒé™"
                )
                return

            # Format and send the PR details
            result = self._format_pr_details(repo, pr_data)
            yield event.plain_result(result)

            # Send the PR card image if available
            if pr_data.get("html_url"):
                hash_value = uuid.uuid4().hex
                url_path = pr_data["html_url"].replace("https://github.com/", "")
                card_url = GITHUB_REPO_OPENGRAPH.format(
                    hash=hash_value, appendix=url_path
                )
                try:
                    yield event.image_result(card_url)
                except Exception as e:
                    logger.error(f"ä¸‹è½½ PR å¡ç‰‡å›¾ç‰‡å¤±è´¥: {e}")

        except Exception as e:
            logger.error(f"è·å– PR è¯¦æƒ…æ—¶å‡ºé”™: {e}")
            yield event.plain_result(f"è·å– PR è¯¦æƒ…æ—¶å‡ºé”™: {str(e)}")

    def _parse_issue_reference(
        self, reference: str, msg_origin: str = None
    ) -> Tuple[Optional[str], Optional[str]]:
        """Parse issue/PR reference string in various formats"""
        # Try format 'owner/repo#number' or 'owner/repo number'
        match = re.match(r"([\w\-]+/[\w\-]+)(?:#|\s+)(\d+)$", reference)
        if match:
            return match.group(1), match.group(2)

        # Try format 'owner/repo/number' (without spaces)
        match = re.match(r"([\w\-]+/[\w\-]+)/(\d+)$", reference)
        if match:
            return match.group(1), match.group(2)

        # If reference is just a number, try to use default repo or a subscribed repo
        if reference.isdigit():
            # First check for default repo for this conversation
            if msg_origin and msg_origin in self.default_repos:
                return self.default_repos[msg_origin], reference

            # Next check if there's exactly one subscription
            if msg_origin:
                user_subscriptions = []
                for repo, subscribers in self.subscriptions.items():
                    if msg_origin in subscribers:
                        user_subscriptions.append(repo)

                if len(user_subscriptions) == 1:
                    return user_subscriptions[0], reference
                elif len(user_subscriptions) > 1:
                    logger.debug(
                        f"Found multiple subscriptions for {msg_origin}, can't determine default repo"
                    )

        return None, None
        
    def _parse_readme_reference(self, reference: str) -> Optional[str]:
        """Parse readme reference string."""
        # Match 'owner/repo' and optional '#...' or ' ...' part
        match = re.match(r"([\w\-]+/[\w\-]+)", reference)
        if match:
            return match.group(1)
        return None

    @filter.command("ghreadme")
    async def get_readme_details(self, event: AstrMessageEvent, readme_ref: str):
        """æŸ¥è¯¢æŒ‡å®šä»“åº“çš„ README ä¿¡æ¯ã€‚ä¾‹å¦‚: /ghreadme ç”¨æˆ·å/ä»“åº“å"""
        repo = self._parse_readme_reference(readme_ref)
        if not repo:
            yield event.plain_result(
                "è¯·æä¾›æœ‰æ•ˆçš„ä»“åº“å¼•ç”¨ï¼Œæ ¼å¼ä¸ºï¼šç”¨æˆ·å/ä»“åº“å"
            )
            return

        try:
            readme_data = await self._fetch_readme_data(repo)
            if not readme_data:
                yield event.plain_result(
                    f"æ— æ³•è·å–ä»“åº“ {repo} çš„ README ä¿¡æ¯ï¼Œå¯èƒ½ä¸å­˜åœ¨æˆ–æ— è®¿é—®æƒé™"
                )
                return
            
            # Decode content from base64
            content_base64 = readme_data.get("content", "")
            try:
                readme_content = base64.b64decode(content_base64).decode("utf-8")
            except Exception as e:
                logger.error(f"è§£ç  README å†…å®¹å¤±è´¥: {e}")
                yield event.plain_result(f"è§£ç ä»“åº“ {repo} çš„ README å†…å®¹æ—¶å‡ºé”™")
                return

            # **[REMOVED]** Truncation logic is removed.
            
            header = f"ğŸ“– {repo} çš„ README\n\n"
            full_text = header + readme_content

            # Render text to image
            try:
                image_url = await self.text_to_image(full_text)
                yield event.image_result(image_url)
            except Exception as e:
                logger.error(f"æ¸²æŸ“ README å›¾ç‰‡å¤±è´¥: {e}")
                # Fallback to plain text if image rendering fails
                yield event.plain_result(full_text)

        except Exception as e:
            logger.error(f"è·å– README è¯¦æƒ…æ—¶å‡ºé”™: {e}")
            yield event.plain_result(f"è·å– README è¯¦æƒ…æ—¶å‡ºé”™: {str(e)}")


    async def _fetch_readme_data(self, repo: str) -> Optional[Dict]:
        """Fetch README data from GitHub API"""
        async with aiohttp.ClientSession() as session:
            try:
                url = GITHUB_README_API_URL.format(repo=repo)
                async with session.get(url, headers=self._get_github_headers()) as resp:
                    if resp.status == 200:
                        return await resp.json()
                    else:
                        logger.error(
                            f"è·å– README {repo} å¤±è´¥: {resp.status}"
                        )
                        return None
            except Exception as e:
                logger.error(f"è·å– README {repo} æ—¶å‡ºé”™: {e}")
                return None

    async def _fetch_issue_data(self, repo: str, issue_number: str) -> Optional[Dict]:
        """Fetch issue data from GitHub API"""
        async with aiohttp.ClientSession() as session:
            try:
                url = GITHUB_ISSUE_API_URL.format(repo=repo, issue_number=issue_number)
                async with session.get(url, headers=self._get_github_headers()) as resp:
                    if resp.status == 200:
                        return await resp.json()
                    else:
                        logger.error(
                            f"è·å– Issue {repo}#{issue_number} å¤±è´¥: {resp.status}"
                        )
                        return None
            except Exception as e:
                logger.error(f"è·å– Issue {repo}#{issue_number} æ—¶å‡ºé”™: {e}")
                return None

    async def _fetch_pr_data(self, repo: str, pr_number: str) -> Optional[Dict]:
        """Fetch PR data from GitHub API"""
        async with aiohttp.ClientSession() as session:
            try:
                url = GITHUB_PR_API_URL.format(repo=repo, pr_number=pr_number)
                async with session.get(url, headers=self._get_github_headers()) as resp:
                    if resp.status == 200:
                        return await resp.json()
                    else:
                        logger.error(f"è·å– PR {repo}#{pr_number} å¤±è´¥: {resp.status}")
                        return None
            except Exception as e:
                logger.error(f"è·å– PR {repo}#{pr_number} æ—¶å‡ºé”™: {e}")
                return None

    def _format_issue_details(self, repo: str, issue_data: Dict) -> str:
        """Format issue data for display"""
        # Handle potential PR that was returned from the issues endpoint
        if "pull_request" in issue_data:
            return f"#{issue_data['number']} æ˜¯ä¸€ä¸ª PRï¼Œè¯·ä½¿ç”¨ /ghpr å‘½ä»¤æŸ¥çœ‹è¯¦æƒ…"

        # Parse the datetime and convert to local time for display
        created_str = issue_data["created_at"].replace("Z", "+00:00")
        updated_str = issue_data["updated_at"].replace("Z", "+00:00")

        created_at = datetime.fromisoformat(created_str)
        updated_at = datetime.fromisoformat(updated_str)

        status = "å¼€å¯" if issue_data["state"] == "open" else "å·²å…³é—­"
        labels = ", ".join([label["name"] for label in issue_data.get("labels", [])])

        result = (
            f"ğŸ” Issue è¯¦æƒ… | {repo}#{issue_data['number']}\n"
            f"æ ‡é¢˜: {issue_data['title']}\n"
            f"çŠ¶æ€: {status}\n"
            f"åˆ›å»ºè€…: {issue_data['user']['login']}\n"
            f"åˆ›å»ºæ—¶é—´: {created_at.strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"æ›´æ–°æ—¶é—´: {updated_at.strftime('%Y-%m-%d %H:%M:%S')}\n"
        )

        if labels:
            result += f"æ ‡ç­¾: {labels}\n"

        if issue_data.get("assignees") and len(issue_data["assignees"]) > 0:
            assignees = ", ".join(
                [assignee["login"] for assignee in issue_data["assignees"]]
            )
            result += f"æŒ‡æ´¾ç»™: {assignees}\n"

        if issue_data.get("body"):
            # Truncate long body text
            body = issue_data["body"]
            if len(body) > 200:
                body = body[:197] + "..."
            result += f"\nå†…å®¹æ¦‚è¦:\n{body}\n"

        result += f"\né“¾æ¥: {issue_data['html_url']}"
        return result

    def _format_pr_details(self, repo: str, pr_data: Dict) -> str:
        """Format PR data for display"""
        # Parse the datetime and convert to local time for display
        created_str = pr_data["created_at"].replace("Z", "+00:00")
        updated_str = pr_data["updated_at"].replace("Z", "+00:00")

        created_at = datetime.fromisoformat(created_str)
        updated_at = datetime.fromisoformat(updated_str)

        status = pr_data["state"]
        if status == "open":
            status = "å¼€å¯"
        elif status == "closed":
            status = "å·²å…³é—­" if not pr_data.get("merged") else "å·²åˆå¹¶"

        labels = ", ".join([label["name"] for label in pr_data.get("labels", [])])

        result = (
            f"ğŸ”€ PR è¯¦æƒ… | {repo}#{pr_data['number']}\n"
            f"æ ‡é¢˜: {pr_data['title']}\n"
            f"çŠ¶æ€: {status}\n"
            f"åˆ›å»ºè€…: {pr_data['user']['login']}\n"
            f"åˆ›å»ºæ—¶é—´: {created_at.strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"æ›´æ–°æ—¶é—´: {updated_at.strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"åˆ†æ”¯: {pr_data['head']['label']} â†’ {pr_data['base']['label']}\n"
        )

        if labels:
            result += f"æ ‡ç­¾: {labels}\n"

        if (
            pr_data.get("requested_reviewers")
            and len(pr_data["requested_reviewers"]) > 0
        ):
            reviewers = ", ".join(
                [reviewer["login"] for reviewer in pr_data["requested_reviewers"]]
            )
            result += f"å®¡é˜…è€…: {reviewers}\n"

        if pr_data.get("assignees") and len(pr_data["assignees"]) > 0:
            assignees = ", ".join(
                [assignee["login"] for assignee in pr_data["assignees"]]
            )
            result += f"æŒ‡æ´¾ç»™: {assignees}\n"

        result += (
            f"å¢åŠ : +{pr_data.get('additions', 0)} è¡Œ\n"
            f"åˆ é™¤: -{pr_data.get('deletions', 0)} è¡Œ\n"
            f"æ–‡ä»¶å˜æ›´: {pr_data.get('changed_files', 0)} ä¸ª\n"
        )

        if pr_data.get("body"):
            # Truncate long body text
            body = pr_data["body"]
            if len(body) > 200:
                body = body[:197] + "..."
            result += f"\nå†…å®¹æ¦‚è¦:\n{body}\n"

        result += f"\né“¾æ¥: {pr_data['html_url']}"
        return result

    @filter.command("ghlimit", alias={"ghrate"})
    async def check_rate_limit(self, event: AstrMessageEvent):
        """æŸ¥çœ‹ GitHub API é€Ÿç‡é™åˆ¶çŠ¶æ€"""
        try:
            rate_limit_data = await self._fetch_rate_limit()
            if not rate_limit_data:
                yield event.plain_result("æ— æ³•è·å– GitHub API é€Ÿç‡é™åˆ¶ä¿¡æ¯")
                return

            # Format and send the rate limit details
            result = self._format_rate_limit(rate_limit_data)
            yield event.plain_result(result)

        except Exception as e:
            logger.error(f"è·å– API é€Ÿç‡é™åˆ¶ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            yield event.plain_result(f"è·å– API é€Ÿç‡é™åˆ¶ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")

    async def _fetch_rate_limit(self) -> Optional[Dict]:
        """Fetch rate limit information from GitHub API"""
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(
                    GITHUB_RATE_LIMIT_URL, headers=self._get_github_headers()
                ) as resp:
                    if resp.status == 200:
                        return await resp.json()
                    else:
                        logger.error(f"è·å– API é€Ÿç‡é™åˆ¶ä¿¡æ¯å¤±è´¥: {resp.status}")
                        return None
            except Exception as e:
                logger.error(f"è·å– API é€Ÿç‡é™åˆ¶ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                return None

    def _format_rate_limit(self, rate_limit_data: Dict) -> str:
        """Format rate limit data for display"""
        if not rate_limit_data or "resources" not in rate_limit_data:
            return "è·å–åˆ°çš„é€Ÿç‡é™åˆ¶æ•°æ®æ— æ•ˆ"

        resources = rate_limit_data["resources"]
        core = resources.get("core", {})
        search = resources.get("search", {})
        graphql = resources.get("graphql", {})

        # Convert timestamps to datetime objects
        core_reset = datetime.fromtimestamp(core.get("reset", 0))
        search_reset = datetime.fromtimestamp(search.get("reset", 0))
        graphql_reset = datetime.fromtimestamp(graphql.get("reset", 0))

        # Calculate time until reset
        now = datetime.now()
        core_minutes = max(0, (core_reset - now).total_seconds() // 60)
        search_minutes = max(0, (search_reset - now).total_seconds() // 60)
        graphql_minutes = max(0, (graphql_reset - now).total_seconds() // 60)

        # Format the result
        result = (
            "ğŸ“Š GitHub API é€Ÿç‡é™åˆ¶çŠ¶æ€\n\n"
            "ğŸ’» æ ¸å¿ƒ API (repositories, issues, etc):\n"
            f"  å‰©ä½™è¯·æ±‚æ•°: {core.get('remaining', 0)}/{core.get('limit', 0)}\n"
            f"  é‡ç½®æ—¶é—´: {core_reset.strftime('%H:%M:%S')} (çº¦ {int(core_minutes)} åˆ†é’Ÿå)\n\n"
            "ğŸ” æœç´¢ API:\n"
            f"  å‰©ä½™è¯·æ±‚æ•°: {search.get('remaining', 0)}/{search.get('limit', 0)}\n"
            f"  é‡ç½®æ—¶é—´: {search_reset.strftime('%H:%M:%S')} (çº¦ {int(search_minutes)} åˆ†é’Ÿå)\n\n"
            "ğŸ“ˆ GraphQL API:\n"
            f"  å‰©ä½™è¯·æ±‚æ•°: {graphql.get('remaining', 0)}/{graphql.get('limit', 0)}\n"
            f"  é‡ç½®æ—¶é—´: {graphql_reset.strftime('%H:%M:%S')} (çº¦ {int(graphql_minutes)} åˆ†é’Ÿå)\n"
        )

        # Add information about authentication status
        if self.github_token:
            result += "\nâœ… å·²ä½¿ç”¨ GitHub Token è¿›è¡Œèº«ä»½éªŒè¯ï¼Œé€Ÿç‡é™åˆ¶è¾ƒé«˜"
        else:
            result += (
                "\nâš ï¸ æœªä½¿ç”¨ GitHub Tokenï¼Œé€Ÿç‡é™åˆ¶è¾ƒä½ã€‚å¯åœ¨é…ç½®ä¸­æ·»åŠ  Token ä»¥æé«˜é™åˆ¶"
            )

        return result

    # TODO: svg2png
    # @filter.command("ghstar")
    # async def ghstar(self, event: AstrMessageEvent, identifier: str):
    #     '''æŸ¥çœ‹ GitHub ä»“åº“çš„ Star è¶‹åŠ¿å›¾ã€‚å¦‚: /ghstar Soulter/AstrBot'''
    #     url = STAR_HISTORY_URL.format(identifier=identifier)
    #     # download svg
    #     fpath = "data/temp/{identifier}.svg".format(identifier=identifier.replace("/",
    #         "_"))
    #     await download_file(url, fpath)
    #     # convert to png
    #     png_fpath = fpath.replace(".svg", ".png")
    #     cairosvg.svg2png(url=fpath, write_to=png_fpath)
    #     # send image
    #     yield event.image_result(png_fpath)

    async def terminate(self):
        """Cleanup and save data before termination"""
        self._save_subscriptions()
        self._save_default_repos()
        self.task.cancel()
        logger.info("GitHub Cards Plugin å·²ç»ˆæ­¢")
